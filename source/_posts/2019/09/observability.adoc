---
layout: post
title:  "Monitoring Spring Boot service with Prometheus and Grafana"
date: 2019-09-11 22:07:41
updated: 2019-10-10 22:07:41
tags:
    - Observability
    - Spring Boot
    - Prometheus
    - Grafana
    - Docker
categories:
    - Observability
    - Spring Boot
    - Prometheus
    - Grafana
    - Docker
og_image: /images/bg-index.jpg
eyeCatchImage: /images/bg-index.jpg
---

:micrometer-demo-url: https://github.com/zghurskyi/investigations/tree/master/investigation-micrometer
:spring-boot-prometheus-reference-url: https://docs.spring.io/spring-boot/docs/current/reference/html/production-ready-metrics.html#production-ready-metrics-export-prometheus
:prometheus-overview-reference-url: https://prometheus.io/docs/introduction/overview/
:prometheus-config-reference-url: https://prometheus.io/docs/prometheus/latest/configuration/configuration/
:grafana-datasources-reference-url: https://grafana.com/docs/administration/provisioning/#datasources
:grafana-reference-url: https://grafana.com/docs/v4.3/
:grafana-concepts-url: https://grafana.com/docs/guides/basic_concepts/
:jvm-gc-dashboard-url: https://github.com/zghurskyi/investigations/blob/master/investigation-micrometer/monitoring/dashboards/jvmgc-dashboard.json
:latency-dashboard-url: https://github.com/zghurskyi/investigations/blob/master/investigation-micrometer/monitoring/dashboards/latency-dashboard.json
:cpu-dashboard-url: https://github.com/zghurskyi/investigations/blob/master/investigation-micrometer/monitoring/dashboards/processor-dashboard.json
:grafana-dashboard-reference-url: https://grafana.com/docs/administration/provisioning/#dashboards
:apache-benchmark-reference-url: https://httpd.apache.org/docs/2.4/programs/ab.html

Different people mean different things, when they talk about `observability`.
To stay practical and concrete, by observability I mean `monitoring, tracing and logging`.
In this post I will give recipe of adding monitoring to your Spring Boot service with Prometheus and Grafana.

*TL;DR* If you prefer reading code vs reading posts -- just follow this {micrometer-demo-url}[link].

++++
<!-- more -->
++++

== What will we build?

I will follow simple plan:

. Setup vanilla Spring Boot service (straight from https://start.spring.io)
. Setup Prometheus
. Setup Grafana
. Perform some basic load testing and observe results

So, let's start.

== 1. Bootstrapping Spring Boot service

[source,shell]
----
$ curl https://start.spring.io/starter.zip \
-d dependencies=actuator,webflux,lombok \
-d type=maven-project \
-d baseDir=service \
-d groupId=com.oxymorus.monitoring \
-d artifactId=service \
-d bootVersion=2.1.9.RELEASE \
-o service.zip

$ unzip service.zip && rm service.zip && cd service
----

After bootstrapping bare-bones service, let's prepare next steps by creating `monitoring` directory.
This will be our base directory, where we place scripts and configs for Prometheus and Grafana:

[source,shell]
----
$ mkdir monitoring && cd monitoring
----

== 2. Setting-up Prometheus

NOTE: {prometheus-overview-reference-url}[Prometheus] is an open-source systems monitoring and alerting toolkit.

Spring Boot provides monitoring data in Promethues format through {spring-boot-prometheus-reference-url}[/actuator/prometheus] endpoint.
So, first, we need to expose this endpoint in `application.yml`:

[source,yaml]
----
management:
  endpoints:
    web:
      exposure:
        include:
          - prometheus
----

Next, we need to configure scrapping of exposed endpoint by Prometheus.
To do this, we provide {prometheus-config-reference-url}[prometheus.yml]:

[source,yaml]
----
# my global config
global:
  scrape_interval:     15s # Set the scrape interval to every 15 seconds. Default is every 1 minute.
  evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute.
  # scrape_timeout is set to the global default (10s).

# Load rules once and periodically evaluate them according to the global 'evaluation_interval'.
rule_files:
# - "first_rules.yml"
# - "second_rules.yml"

# A scrape configuration containing exactly one endpoint to scrape:
# Here it's Prometheus itself.
scrape_configs:
  # The job name is added as a label `job=<job_name>` to any timeseries scraped from this config.
  - job_name: 'prometheus'
    # metrics_path defaults to '/metrics'
    # scheme defaults to 'http'.
    static_configs:
      - targets: ['127.0.0.1:9090']

  - job_name: 'spring-actuator'
    metrics_path: '/actuator/prometheus'
    scrape_interval: 5s
    static_configs:
      - targets: ['127.0.0.1:8080']
----

Finally, we can launch Promethues with Docker using following script:

[source,shell script]
----
#!/bin/sh
docker run --net=host -p 9090:9090 \
-v $(pwd)/prometheus.yml:/etc/prometheus/prometheus.yml \
prom/prometheus:v2.2.0
----

NOTE: Don't forget to make script executable with `chmod +x prometheus.sh`

The script starts Prometheus on port 9090
and configures it to scrape Spring Boot `/actuator/prometheus` endpoint.

We can check that everything is working by visiting default dashboard at `localhost:9090`:

[.text-center]
--
[.img-responsive.img-thumbnail]
[link=/images/prometheus-dashboard.png]
image::/images/prometheus-dashboard.png[]
--

== 3. Setting-up Grafana

NOTE: {grafana-reference-url}[Grafana] is an open source metric analytics & visualization suite.
It is most commonly used for visualizing time series data for infrastructure
and application analytics but many use it in other domains
including industrial sensors, home automation, weather, and process control.

Grafana uses {grafana-concepts-url}[dashboards] to organize your monitoring/metrics visualisation.
So, we will preconfigure several dashboards:

- {jvm-gc-dashboard-url}[JVM garbage collection stats]

- {latency-dashboard-url}[Latency stats -- max & 99th percentile]

- {cpu-dashboard-url}[Processor/CPU load]

To provision these dashboards we need to provide {grafana-dashboard-reference-url}[grafana-dashboard.yml]:

[source,yaml]
----
apiVersion: 1

providers:
  - name: 'default'
    folder: 'Spring Boot'
    type: file
    options:
      path: /etc/grafana/dashboards
----

Next, to connect Grafana with Prometheus as its datasource
we should provide {grafana-datasources-reference-url}[grafana-datasource.yml]:

[source,yaml]
----
apiVersion: 1

datasources:
  - name: prometheus
    type: prometheus
    access: direct
    url: http://127.0.0.1:9090
----

Finally, after all preparation we can start Grafana with following script:

[source,shell script]
----
#!/bin/sh
docker run -i --net=host \
-p 3000:3000 \
-v $(pwd)/grafana-datasource.yml:/etc/grafana/provisioning/datasources/grafana-datasource.yml \
-v $(pwd)/dashboards/grafana-dashboard.yml:/etc/grafana/provisioning/dashboards/grafana-dashboard.yml \
-v $(pwd)/dashboards/jvmgc-dashboard.json:/etc/grafana/dashboards/jvmgc.json \
-v $(pwd)/dashboards/latency-dashboard.json:/etc/grafana/dashboards/latency.json \
-v $(pwd)/dashboards/processor-dashboard.json:/etc/grafana/dashboards/processor.json \
grafana/grafana:5.1.0
----

NOTE: Don't forget to make script executable with `chmod +x prometheus.sh`

The script starts Grafana on `localhost:3000`.

NOTE: To login use default admin/admin credentials.

To verify everything is working, check preconfigured JVM GC dashboard:

[.text-center]
--
[.img-responsive.img-thumbnail]
[link=/images/grafana-dashboard.png]
image::/images/grafana-dashboard.png[]
--

== 4. Perform some basic load testing and observe results

After service is running and monitoring is properly configured,
we can perform some load testing and observe how service behaves.

For load testing we will use simple command line utility {apache-benchmark-reference-url}[Apache Benchmark].

[source,shell script]
----
$ ab -n 1000000 -c 10 http://localhost:8080/actuator/prometheus
----

This command performs 1 million requests in 10 concurrent threads to the `http://localhost:8080/actuator/prometheus`.

So, it's time to observe some results:

[.text-center]
--
[.img-responsive.img-thumbnail]
[caption="Heap utilization"]
[link=/images/grafana-heap-utilization.png]
image::/images/grafana-heap-utilization.png[]
--

[.text-center]
--
[.img-responsive.img-thumbnail]
[caption="Average GC pause time"]
[link=/images/grafana-average-gc-pause-time.png]
image::/images/grafana-average-gc-pause-time.png[]
--

[.text-center]
--
[.img-responsive.img-thumbnail]
[caption="Max Latency by endpoint"]
[link=/images/grafana-max-latency-by-endpoint.png]
image::/images/grafana-max-latency-by-endpoint.png[]
--

[.text-center]
--
[.img-responsive.img-thumbnail]
[caption="Request Throughput"]
[link=/images/grafana-request-throughput.png]
image::/images/grafana-request-throughput.png[]
--

[.text-center]
--
[.img-responsive.img-thumbnail]
[caption="CPU load"]
[link=/images/grafana-cpu-load.png]
image::/images/grafana-cpu-load.png[]
--

== Conclusion

It's actually straight-forward to setup some basic Prometheus/Grafana monitoring,
since all tools are already in place and fit together pretty well.

Available tools allow to get comprehensive view of the system.

Next step, after having this harness in place, is
to configure custom metrics with Micrometer and make sense from all of them.
I'm saving it for the next time, so stay tuned!